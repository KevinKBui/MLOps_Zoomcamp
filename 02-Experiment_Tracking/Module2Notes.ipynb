{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96c53704-9b8e-4520-bf39-a659eee6db1c",
   "metadata": {},
   "source": [
    "# Notes: Overview\n",
    "MLFlow uses/terminology:\n",
    "-  ML experiment: the process of building an ML model (playing with models/hyperparameters).\n",
    "- Experiment run: each trial in an ML experiment\n",
    "- Run artifact: any file that is associated with an ML run\n",
    "- Experiment metadata: info relating to experiment.\n",
    "- Experiment tracking: Source code, Environment, Data, Model, Hyperparameters, Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9387fd8d-b037-407a-84f4-559df64e9f7f",
   "metadata": {},
   "source": [
    "# Notes: MLFlow\n",
    "Information Manually Logged:\n",
    "- MLFlow: Tracking (experiment tracking), MLFlow Models (special type of models), Model Registry, MLFlow Projects (not in scope).\n",
    "MLFlow tracks the following per run:\n",
    "- Parameters (hyperparameter of model, parameters that could affect the model ie path to the training dataset, preprocessing to input data). It can be reflected in the experiment that you were playing with different versions of data, for example.\n",
    "-  Metrics: evaluation metrics (accuracy, F1, precision, etc). Metrics can be from training, validation, or test datasets.\n",
    "-  Metadata: any info related to the experiment (ie tags so that you can search/filter runs).\n",
    "-  Artifacts: any files logged that are supplementary (ie visualization). Dataset not recommended to be logged as artifact since we can have duplicates.\n",
    "-  Models: log the model of the experiment.\n",
    "Information Automatically Logged (by MLFlow):\n",
    "- Source code (name of the file)\n",
    "- Version of code (git commit)\n",
    "- Start/End Time\n",
    "- Author "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c684cc-24c1-4b6e-8666-eebf7d33dc22",
   "metadata": {},
   "source": [
    "# MLFlow Demo:\n",
    "- Get access to MLFlow CLI once it is installed\n",
    "# MLFlow syntax:\n",
    "mlflow [Options] Command [Arguments]\n",
    "- Options = --version, --help\n",
    "- Command = artifacts, azureml, db, deployments, experiments, gc, models, run, runs, sagemaker, server, ui \n",
    "- Type mlflow to get more descriptions\n",
    "MLFlow UI: \n",
    "- A link will be displayed. go to the address to get access to experiments\n",
    "- Experiments will all be listed on lefthand side.\n",
    "- When creating experiment, it will ask for artifact location (optional) where you can specify the location of all your artifacts.\n",
    "-  Models tab = Model Registry (error will be returned if running ui without any backend. Need to specify sql database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6345b6e9-b34b-461f-85ad-4d15f99a7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tells mlflow we want to store all artifacts and metadata in sqlite (one alternative of backend storage).\n",
    "# This is necessary to use model registry or else we will run into the same error previously mentioned.\n",
    "mlflow ui --backend-store-uri sqlite:///mlflow.db"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40abd4a6-bdf8-4584-9b8f-0f15786859d9",
   "metadata": {},
   "source": [
    "# Module 2.2 Notes:\n",
    "Achieved:\n",
    "- Preparing the local environment, installing mlflow client\n",
    "- Starting up mlflow experiment\n",
    "- Navigating mlflow ui (looking at results, etc).\n",
    "- Adding mlflow to notebook, logging information (parameters and metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cfd530-a573-4727-97bd-649e5d3235e6",
   "metadata": {},
   "source": [
    "# Module 2.3 Notes:\n",
    "- Add hyperparameter tuning to notebook and how to explore results of hyperparameter search in mlflow ui\n",
    "- Discussing how to select the best model based on the results\n",
    "- Autologging (enables logging with less lines of code)\n",
    "- Selecting multiple runs allows us to compare the results between runs.\n",
    "- To filter by tags, use tags.key = 'value'\n",
    "- Parallel coordinate plots can tell you which combinations of hyperparameters perform the best and worst\n",
    "- Scatterplot gives you a direct correlation between a hyperparameter and the metric (ie rmse).\n",
    "- Contour plot allows you to see 2 hyperparameter relationships with each other and against the metric as well (metric being the elevation).\n",
    "- Since we want to minimize error in this example, we reverse the color to find the minimal point (the minimal error).\n",
    "\n",
    "Automatic Logging: Works only with certain frameworks (can be found on mlflow auto log page/below).\n",
    "- The following libraries support autologging:\n",
    "- Scikit-learn\n",
    "- TensorFlow and Keras\n",
    "- Slupn\n",
    "- XGBoost|\n",
    "- LightGBM\n",
    "- Statsmodels\n",
    "- Spark\n",
    "- Fastai\n",
    "- Pytorch\n",
    "\n",
    "AutoLogging using XGBoost:\n",
    "- saves visualization about feature importance of the model\n",
    "- saves info about the importance of the features\n",
    "- saves model as an mlflow model (model.xgb in this case).\n",
    "- saves dependencies (library and library versions), conda environments, pip and python version, etc.\n",
    "- saves run information (can run it as a python function or as an XGBoost model).\n",
    "- Clicking on model gives you examples on how to load the models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9485aa-b055-4508-ad66-62d516056a39",
   "metadata": {},
   "source": [
    "# Module 2.4 Notes: Model Management\n",
    "- Source for MLOps lifecycle diagram: https://neptune.ai/blog/ml-experiment-tracking\n",
    "- Experiment tracking is only a part of MLOps life cycle. There's also Model Management (which experiment tracking is a part of).\n",
    "- Model Management covers: Experiment tracking, Model versioning, Model deployment, Scaling hardware\n",
    "- After experiment tracking, we are happy with the model. We then need to consider ways to save and version the models. Then we want to deploy the model and then potentially update the model in order to scale it.\n",
    "- The last stage of the MLOps life cycle (after model management) is Prediction Monitoring.\n",
    "- Most basic version of saving a model in mlflow - saving it as an artifact\n",
    "- mlflow.log_artifact(local_path=location where the artifact is located, artifact_path= where the model will be saved)\n",
    "- mlflow.log_params(dictionary) is another way of logging parameters. Notice params is plural. Pass in a dictionary of parameters and it will log it all.\n",
    "- Second way of logging models with mlflow: mlflow.framework.log_model(model, artifact_path= where to save the model). framework is the framework you're using to train the model ie xgb (xgboost) in this case. model is variable where your model is initialized (booster).\n",
    "- MLModel = mlflow model. Stores info about the model itself: artifact_path = where model is stored, flavors: list of methods to load the model\n",
    "- Save preprocessor as an artifact to later load the preprocessor to preprocess prediction data into the model.\n",
    "To make predictions using the model:\n",
    "- first step is to pass to mlflow the model URI (unique resource identifier). Basically, it's pointing to a run -> run ID -> model folder inside the run's artifact.\n",
    "-  mlflow.pyfunc.library_method('runs:/run_ID/model_name') then used to load the model (the type of pyfunc depends on which framework you are loading the model into ie pandas or spark). These are both examples of loading the model as a python function.\n",
    "-  Second method is to load it into the framework (creating an object from that framework as if you loaded the model into that library): mlflow.framework.load_model('runs:/run_ID/model_name')\n",
    "-  In this example, it was: xgboost_model = mlflow.xgboost.load_model('runs:/088f9cd07a9f49f6b23b141076902c53/models_mlflow')\n",
    "-  By loading the model into the framework, you create an object type of the framework (ie a xgboost type object). THIS IS IMPORTANT BECUSE IT GIVES YOU ACCESS TO THE METHODS OF THE FRAMEWORK OBJECT TYPE (ie xgboost methods).\n",
    "-  After the models are loaded, then you can make predictions by just calling the .predict() method as usual.\n",
    "-  So the order goes load model -> pick load type (python function or framework) -> deploy (ie to a cloud, Spark, kubernetes clusters, docker, jupyter notebooks, etc)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2321c8ff-f287-4432-8b8c-4ff47b7099a0",
   "metadata": {},
   "source": [
    "# Module 2.5 Notes:\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a3ece0-a0ec-4fd0-869b-4b34f0e734f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
